شرح الشريحة السابعة (المعلومة الذاتية والحل العام) - بتنسيق المعادلات المفصولالنص:$\rhd$ Average Information Contentالشرح:بعد أن أدركنا أن $H_0$ غير كافٍ، ننتقل الآن إلى إيجاد معادلة عامة لقياس المعلومة الذاتية للرمز ($I(x)$) والتي تؤدي بنا إلى حساب الإنتروبيا (المتوسط).النص:Source alphabet: $X = \{x_1, \dots, x_N\}$الشرح:أبجدية المصدر ($X$) هي المجموعة الكاملة لجميع الرموز الممكنة التي يمكن أن يولدها المصدر، من $x_1$ حتى $x_N$.النص:Probabilities of the symbols: $p(x_1), \dots, p(x_N)$الشرح:لكل رمز، توجد احتمالية ظهور مقابلة $p(x_i)$. مجموع هذه الاحتمالات يجب أن يساوي $1$.النص:Desired properties of the information content $I = f(p)$:الشرح:يجب أن تحقق دالة محتوى المعلومات ($I$) التي تعتمد على الاحتمالية ($p$) الخصائص المنطقية التالية:النص:$I(x_i) \geq 0 \text{ for } 0 \leq p(x_i) \leq 1$الشرح:المعلومة لا يمكن أن تكون سالبة. يجب أن تكون $\boldsymbol{I(x_i)}$ أكبر من أو تساوي الصفر:$$I(x_i) \geq 0 \text{ for } 0 \leq p(x_i) \leq 1$$النص:$I(x_i) = 0 \text{ for } p(x_i) = 1$الشرح:إذا كان احتمال وقوع الحدث مؤكد الحدوث ($p(x_i)=1$)، فإن محتوى المعلومات لهذا الحدث يساوي صفر:$$I(x_i) = 0 \text{ for } p(x_i) = 1$$النص:$I(x_i) > I(x_j) \text{ for } p(x_i) < p(x_j)$الشرح:هنا تظهر خاصية التناسب العكسي: كلما كانت احتمالية ظهور الرمز $\boldsymbol{p(x_i)}$ أقل، كلما كان محتوى المعلومات $\boldsymbol{I(x_i)}$ أكبر:$$I(x_i) > I(x_j) \text{ for } p(x_i) < p(x_j)$$النص:The joint probability of two statistically independent symbols $x_i$ $\&$ $x_j$ is: $p(x_i, x_j) = p(x_i) \cdot p(x_j)$, the joint information content $I(x_i, x_j) = I(x_i) + I(x_j)$الشرح:بالنسبة لحدثين مستقلين إحصائياً، يجب أن يكون احتمال وقوعهما المشترك هو:$$p(x_i, x_j) = p(x_i) \cdot p(x_j)$$بينما يكون محتوى المعلومات المشترك هو مجموع محتويات معلوماتهما الفردية (خاصية الجمع للوغاريتم):$$I(x_i, x_j) = I(x_i) + I(x_j)$$النص:General solution : $I(x_i) = -k \cdot \log_b(p(x_i))$الشرح:الصيغة الرياضية الوحيدة التي تحقق جميع الخصائص المطلوبة هي دالة اللوغاريتم، والحل العام لمحتوى المعلومة هو:$$I(x_i) = -k \cdot \log_b(p(x_i))$$