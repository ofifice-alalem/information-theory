شرح الشريحة الثامنة (التعريفات النهائية للإنتروبيا)النص:$\rhd$ Average Information Contentالشرح:بعد أن أثبتنا في الشريحة السابقة أن المعلومة الذاتية يجب أن تكون دالة لوغاريتمية تتناسب عكسياً مع الاحتمالية، نحدد الآن الصيغة القياسية.النص:Definition of the information content:$$I(x_i) = \text{ld}\left(\frac{1}{p(x_i)}\right) \text{ bit/symbol}$$الشرح:يُعرف محتوى المعلومة (Information Content) للرمز $x_i$ (أو المعلومة الذاتية) بالصيغة أعلاه. وقد تم اختيار:الأساس ($b$): يساوي 2، لذا نستخدم $\text{ld}$ (logarithm dualis)، لتكون الوحدة هي البت (bit).الثابت ($k$): يساوي 1، لتبسيط المعادلة.علامة السالب: يتم استبدالها بوضع مقلوب الاحتمالية $\left(\frac{1}{p(x_i)}\right)$ داخل اللوغاريتم، لأن اللوغاريتم لمقلوب الاحتمال يساوي سالب لوغاريتم الاحتمال $\left(\log_2 \left(\frac{1}{p}\right) = - \log_2 (p)\right)$.النص:Entropy $H(X) = \overline{I(x_i)}$ = average information content of a source:الشرح:الإنتروبيا $H(X)$ هي متوسط محتوى المعلومات لجميع رموز المصدر. وتساوي القيمة المتوقعة $\overline{I(x_i)}$ للمعلومة الذاتية.النص:$$H(X) = \sum_{i=1}^{N} p(x_i) \cdot I(x_i)$$الشرح:لحساب المتوسط، نستخدم المجموع الوزني. فنضرب المعلومة الذاتية $I(x_i)$ لكل رمز في احتمالية ظهوره $p(x_i)$، ثم نجمع النتائج لجميع الرموز من $i=1$ إلى $N$.النص:$$H(X) = \sum_{i=1}^{N} p(x_i) \cdot \text{ld}\left(\frac{1}{p(x_i)}\right) \text{ bit/symbol}$$الشرح:هذه هي الصيغة النهائية للإنتروبيا، حيث تم استبدال $I(x_i)$ بتعريفها الرياضي $\text{ld}\left(\frac{1}{p(x_i)}\right)$. وتُعد هذه المعادلة هي المقياس الحقيقي لمتوسط عدم اليقين أو محتوى المعلومات في المصدر، ووحدتها هي بت لكل رمز (bit/symbol).