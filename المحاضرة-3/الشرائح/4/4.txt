شرح الشريحة الرابعة (متوسط محتوى المعلومات - الإنتروبيا)

النص:
$\rhd$ Average Information Content
الشرح:
بعد أن عرفنا كيف نقيس المعلومة لرمز واحد (المعلومة الذاتية)، ننتقل الآن لقياس متوسط محتوى المعلومات لمصدر المعلومات بأكمله. وهذا ما يسمى الإنتروبيا (Entropy).

النص:
Information content of a message: Entropy
الشرح:
الإنتروبيا هي المقياس الكمّي الذي يحدد كمية المعلومات التي يولدها مصدر ما في المتوسط، أو ببساطة، هي متوسط عدد البتات اللازمة لترميز كل رمز صادر من المصدر.

النص:
Qualitative ordering of messages: a message is more important if it is more difficult to predict it
الشرح:
هذا هو التفسير النوعي لمفهوم المعلومة: تزداد أهمية (أو قيمة) الرسالة كلما كان التنبؤ بها أصعب. وهذا المبدأ هو أساس علاقة التناسب العكسي بين الأهمية والاحتمالية.

النص:
Example : Tomorrow, the sun rises.
الشرح:
هذه الرسالة محتملة جداً، فاحتمالية وقوعها عالية (قرب الـ 100%). بالتالي، هي تحمل أقل كمية من المعلومات أو أهمية بالنسبة للمستقبل. [الإشارة إلى السهم الأخضر لـ Probability العالي]

النص:
Example : Tomorrow, there will be bad weather.
الشرح:
هذه الرسالة أقل احتمالاً من شروق الشمس، لذا فهي تحمل كمية معلومات أعلى قليلاً، لأنها تتضمن قدراً أكبر من عدم اليقين.

النص:
Example : Tomorrow, there will be a heavy thunderstorm so that the electric power network will break down.
الشرح:
هذه الرسالة نادرة الحدوث وصعبة التنبؤ، فاحتمالية وقوعها منخفضة جداً. ولذلك، فهي تحمل أقصى كمية من المعلومات (أو أعلى أهمية) بالنسبة للمستقبل. [الإشارة إلى السهم الأزرق لـ Importance العالي]

النص:
Messages from a digital source: sequence of symbols
الشرح:
عندما نطبق هذا المفهوم على المصادر الرقمية، فإن الرسائل هي ببساطة تسلسل من الرموز (Symbols) (مثل 0 و 1، أو الأحرف). هدفنا هو حساب متوسط المعلومات التي يحملها كل رمز في هذا التسلسل.